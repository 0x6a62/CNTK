# Train a CTC with uni-directional LSTM, 30 letters as outputs
# 
# Reference: 
# [1] Amit Das, Jinyu Li, Guoli Ye, Rui Zhao, and Yifan Gong, “Advancing Acoustic-to-Word CTC Model with Attention and Mixed-Units,” 
# IEEE Trans. Audio, Speech, Lang. Process., 2019.
#
# 
# Parameter settings for training different kinds of CTCs (Section III in [1]):
# 1. Vanilla CTC:   attnType=4		nDelay= dont'care		prevStateType=0		prevAttnType=0		enableDimAttn=0
# 2. 		+ TC:  	attnType=0		nDelay=4				prevStateType=0		prevAttnType=0		enableDimAttn=0
# 3.  		+ CA:	attnType=3		nDelay=4				prevStateType=1		prevAttnType=0		enableDimAttn=0
# 4.  		+ HA: 	attnType=3		nDelay=4				prevStateType=1		prevAttnType=1		enableDimAttn=0
# 5.  	   + PLM: 	attnType=3		nDelay=4				prevStateType=2		prevAttnType=1		enableDimAttn=0
# 6.	  + COMA:	attnType=3		nDelay=4				prevStateType=2		prevAttnType=1		enableDimAttn=1
#
# Note: In the config below, replace $$ with appropriate paths in your set-up.


ResourceDir=$$
ModelDir=$WorkDir$/Models
ConfigDir= $$

command = updatemodel
deviceId=auto
traceLevel=1

########### Parameters for CTC w/ TC + CA + HA + PLM + COMA ###########
nDelay=4
attnType=3
prevStateType=2
prevAttnType=1
enableDimAttn=1

# Need not change these parameters
nSkip=1
enableBias=true

# Parameters for 3400 hr data
max_epochs=250
LR=0.0001*34:0.00005*34:0.000025*34:0.000012*34:0.000006*34:0.000003*34:0.0000015
scpFile=$$/3400_CTC.nopath_philly_et1.scp
mlfFile=$ResourceDir$/earlier_3400hr_oneletterbase_skip3_nopath.removedup.mlf


##############################################################

parallelTrain=true
nbruttsineachrecurrentiter=60
maxUtteranceLength=500

featDimension=240
precision=float
shareNodeValueMatrices=true

Truncated=false

	
SGD=[
    epochSize=8640000 # 60hrs training per epoch, due to the memory limit in Philly
    minibatchSize=8000	# for BLSTM, has to be 1. 
	
	#1GPU: 0.0005*EPOCH_PER_1000hr:0.00025*(EPOCH_PER_SWEEP-EPOCH_PER_1000hr):0.000125*(2* EPOCH_PER_SWEEP):0.0000625*(3* EPOCH_PER_SWEEP):0.00003125*(3* EPOCH_PER_SWEEP) 
	#n-GPU: *n
    learningRatesPerSample=$LR$
    #learningRatesPerSample=0.0005*20:0.00025*32:0.000125*104:0.0000625*156:0.00003125*156
	
    # 6 full sweeping 
    maxEpochs=$max_epochs$

	
	# momentumPerMB=0.9*10:0.99		# don't use this for BLSTM, because mbsize has to be 1 (which does not really represent how many frames in a minibatch), this makes (a bug?) the effective momentumAsTimeConstant very small, -mbsize/ln(momentPerMB) = -1/ln(0.9) = 9.5 samples), use momentumAsTimeConstant instead
	momentumAsTimeConstant=2500
	
    dropoutRate=0.0
    numMBsToShowResult=100
	
	# Parameter values for the reader
	ParallelTrain = [
		parallelizationMethod = BlockMomentumSGD
		distributedMBReading = true
		#parallelizationStartEpoch = 5
		syncPerfStats = 5
		BlockMomentumSGD=[
			syncPeriod = 120000
			resetSGDMomentum = true
			useNesterovMomentum = true
		]
	]

    gradientClippingWithTruncation=true
    clippingThresholdPerSample=0.01
]

# Parameter values for the reader
reader=[
	
	verbosity = 0

    randomize = true
	randomizationWindow = 15000000
	miniBatchMode=Partial	
		
	#frameMode=false
	deserializers = (
		[
			type = "HTKFeatureDeserializer"
			module = "HTKDeserializers"
			input = [
				# Description of input stream to feed the Input node named "features"
				features = [
					dim=240
					scpFile=$scpFile$
					contextWindow=5
				]
			]
		]:

		[
			type = "HTKMLFDeserializer"
			module = "HTKDeserializers"
			input = [
				# Description of input stream to feed the Input node named "labels"
				letterctclabels = [
					dim = 30
					mlfFile=$mlfFile$
					labelMappingFile=$ResourceDir$/ealier_oneletter_blank.list
					labelType=Category
					phoneBoundaries=true
				]
			]

		]

    )
	

]

updatemodel=[

    action=trainRNN	

    BrainScriptNetworkBuilder = {
    	
    	include "$ConfigDir$/add_oneletter_uniCTC5L_conv_attn.bs"
    	
		net = CTCAttention(nDelay=$nDelay$, nSkip=$nSkip$, attnType=$attnType$, prevStateType=$prevStateType$, prevAttnType=$prevAttnType$, enableDimAttn=$enableDimAttn$, enableBias=$enableBias$);
    	features  = net.features
    	letterctclabels = net.letterctclabels
    	z = net.z

    	graph = LabelsToGraph(letterctclabels)		
		mycr=ForwardBackward ( graph , z, 29, delayConstraint=-1, tag="criterion")  # 30 letter - 1 = 29
        CTCErr=EditDistanceError( graph , z ,squashInputs=true,subPen=10.0, delPen=7.0, insPen=7.0, tokensToIgnore = 29, tag="evaluation") 
		LogPost = LogSoftmax(z)
		
        featureNodes    = (features)
        labelNodes      = (letterctclabels)
        criterionNodes  = (mycr)
        evaluationNodes = (CTCErr)
        outputNodes     = (LogPost)
    }

    modelPath=$ModelDir$/3400hr.oneletter.ctc
]
